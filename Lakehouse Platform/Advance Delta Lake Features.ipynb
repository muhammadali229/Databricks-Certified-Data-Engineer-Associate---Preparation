{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c44b40b8-22da-4edc-a1d4-9f1dec0d9834",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>3</td><td>2024-01-20T16:54:38.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>UPDATE</td><td>Map(predicate -> [\"StartsWith(name#2444, A)\"])</td><td>null</td><td>List(2923202399434373)</td><td>0120-142018-2rzjaj31</td><td>2</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 4, numAddedChangeFiles -> 0, executionTimeMs -> 902, scanTimeMs -> 461, numAddedFiles -> 1, numUpdatedRows -> 2, rewriteTimeMs -> 440)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>2</td><td>2024-01-20T16:54:05.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>UPDATE</td><td>Map(predicate -> [\"StartsWith(name#1844, A)\"])</td><td>null</td><td>List(2923202399434373)</td><td>0120-142018-2rzjaj31</td><td>1</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 4, numAddedChangeFiles -> 0, executionTimeMs -> 1623, scanTimeMs -> 1021, numAddedFiles -> 1, numUpdatedRows -> 2, rewriteTimeMs -> 592)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>1</td><td>2024-01-20T16:44:59.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>WRITE</td><td>Map(mode -> Append, partitionBy -> [])</td><td>null</td><td>List(2923202399434373)</td><td>0120-142018-2rzjaj31</td><td>0</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 1, numOutputRows -> 6, numOutputBytes -> 1155)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>0</td><td>2024-01-20T16:34:24.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>CREATE TABLE</td><td>Map(isManaged -> true, description -> null, partitionBy -> [], properties -> {})</td><td>null</td><td>List(2923202399434373)</td><td>0120-142018-2rzjaj31</td><td>null</td><td>WriteSerializable</td><td>true</td><td>Map()</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         3,
         "2024-01-20T16:54:38.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "UPDATE",
         {
          "predicate": "[\"StartsWith(name#2444, A)\"]"
         },
         null,
         [
          "2923202399434373"
         ],
         "0120-142018-2rzjaj31",
         2,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "902",
          "numAddedChangeFiles": "0",
          "numAddedFiles": "1",
          "numCopiedRows": "4",
          "numRemovedFiles": "1",
          "numUpdatedRows": "2",
          "rewriteTimeMs": "440",
          "scanTimeMs": "461"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         2,
         "2024-01-20T16:54:05.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "UPDATE",
         {
          "predicate": "[\"StartsWith(name#1844, A)\"]"
         },
         null,
         [
          "2923202399434373"
         ],
         "0120-142018-2rzjaj31",
         1,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "1623",
          "numAddedChangeFiles": "0",
          "numAddedFiles": "1",
          "numCopiedRows": "4",
          "numRemovedFiles": "1",
          "numUpdatedRows": "2",
          "rewriteTimeMs": "592",
          "scanTimeMs": "1021"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         1,
         "2024-01-20T16:44:59.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "WRITE",
         {
          "mode": "Append",
          "partitionBy": "[]"
         },
         null,
         [
          "2923202399434373"
         ],
         "0120-142018-2rzjaj31",
         0,
         "WriteSerializable",
         true,
         {
          "numFiles": "1",
          "numOutputBytes": "1155",
          "numOutputRows": "6"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         0,
         "2024-01-20T16:34:24.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "CREATE TABLE",
         {
          "description": null,
          "isManaged": "true",
          "partitionBy": "[]",
          "properties": "{}"
         },
         null,
         [
          "2923202399434373"
         ],
         "0120-142018-2rzjaj31",
         null,
         "WriteSerializable",
         true,
         {},
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "userId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "userName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationParameters",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "job",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "notebook",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "clusterId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "readVersion",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "isolationLevel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isBlindAppend",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "operationMetrics",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "userMetadata",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "engineInfo",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql \n",
    "describe history employees;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2326f0d1-9c40-43da-b00e-b6eb619862b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>salary</th></tr></thead><tbody><tr><td>1</td><td>Adam</td><td>3500.0</td></tr><tr><td>2</td><td>Sarah</td><td>4020.5</td></tr><tr><td>3</td><td>John</td><td>2999.3</td></tr><tr><td>4</td><td>Thomas</td><td>4000.3</td></tr><tr><td>5</td><td>Anna</td><td>2500.0</td></tr><tr><td>6</td><td>Kim</td><td>6200.3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Adam",
         3500.0
        ],
        [
         2,
         "Sarah",
         4020.5
        ],
        [
         3,
         "John",
         2999.3
        ],
        [
         4,
         "Thomas",
         4000.3
        ],
        [
         5,
         "Anna",
         2500.0
        ],
        [
         6,
         "Kim",
         6200.3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM employees VERSION AS OF 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27680425-1b98-47ae-9947-495c20e8d4c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>salary</th></tr></thead><tbody><tr><td>1</td><td>Adam</td><td>3500.0</td></tr><tr><td>2</td><td>Sarah</td><td>4020.5</td></tr><tr><td>3</td><td>John</td><td>2999.3</td></tr><tr><td>4</td><td>Thomas</td><td>4000.3</td></tr><tr><td>5</td><td>Anna</td><td>2500.0</td></tr><tr><td>6</td><td>Kim</td><td>6200.3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Adam",
         3500.0
        ],
        [
         2,
         "Sarah",
         4020.5
        ],
        [
         3,
         "John",
         2999.3
        ],
        [
         4,
         "Thomas",
         4000.3
        ],
        [
         5,
         "Anna",
         2500.0
        ],
        [
         6,
         "Kim",
         6200.3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM employees@v1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddbb4dba-a977-4f1d-ae72-642e4e49f616",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql \n",
    "truncate table employees;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd8ac063-9e55-433c-a762-7146fc3f792e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>salary</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from employees;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5368531e-8f13-4eaf-b688-0f5f325e2c51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>4</td><td>2024-01-20T17:22:23.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>TRUNCATE</td><td>Map()</td><td>null</td><td>List(2923202399434393)</td><td>0120-142018-2rzjaj31</td><td>3</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, executionTimeMs -> 99)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>3</td><td>2024-01-20T16:54:38.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>UPDATE</td><td>Map(predicate -> [\"StartsWith(name#2444, A)\"])</td><td>null</td><td>List(2923202399434373)</td><td>0120-142018-2rzjaj31</td><td>2</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 4, numAddedChangeFiles -> 0, executionTimeMs -> 902, scanTimeMs -> 461, numAddedFiles -> 1, numUpdatedRows -> 2, rewriteTimeMs -> 440)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>2</td><td>2024-01-20T16:54:05.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>UPDATE</td><td>Map(predicate -> [\"StartsWith(name#1844, A)\"])</td><td>null</td><td>List(2923202399434373)</td><td>0120-142018-2rzjaj31</td><td>1</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 4, numAddedChangeFiles -> 0, executionTimeMs -> 1623, scanTimeMs -> 1021, numAddedFiles -> 1, numUpdatedRows -> 2, rewriteTimeMs -> 592)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>1</td><td>2024-01-20T16:44:59.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>WRITE</td><td>Map(mode -> Append, partitionBy -> [])</td><td>null</td><td>List(2923202399434373)</td><td>0120-142018-2rzjaj31</td><td>0</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 1, numOutputRows -> 6, numOutputBytes -> 1155)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>0</td><td>2024-01-20T16:34:24.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>CREATE TABLE</td><td>Map(isManaged -> true, description -> null, partitionBy -> [], properties -> {})</td><td>null</td><td>List(2923202399434373)</td><td>0120-142018-2rzjaj31</td><td>null</td><td>WriteSerializable</td><td>true</td><td>Map()</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         4,
         "2024-01-20T17:22:23.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "TRUNCATE",
         {},
         null,
         [
          "2923202399434393"
         ],
         "0120-142018-2rzjaj31",
         3,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "99",
          "numRemovedFiles": "1"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         3,
         "2024-01-20T16:54:38.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "UPDATE",
         {
          "predicate": "[\"StartsWith(name#2444, A)\"]"
         },
         null,
         [
          "2923202399434373"
         ],
         "0120-142018-2rzjaj31",
         2,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "902",
          "numAddedChangeFiles": "0",
          "numAddedFiles": "1",
          "numCopiedRows": "4",
          "numRemovedFiles": "1",
          "numUpdatedRows": "2",
          "rewriteTimeMs": "440",
          "scanTimeMs": "461"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         2,
         "2024-01-20T16:54:05.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "UPDATE",
         {
          "predicate": "[\"StartsWith(name#1844, A)\"]"
         },
         null,
         [
          "2923202399434373"
         ],
         "0120-142018-2rzjaj31",
         1,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "1623",
          "numAddedChangeFiles": "0",
          "numAddedFiles": "1",
          "numCopiedRows": "4",
          "numRemovedFiles": "1",
          "numUpdatedRows": "2",
          "rewriteTimeMs": "592",
          "scanTimeMs": "1021"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         1,
         "2024-01-20T16:44:59.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "WRITE",
         {
          "mode": "Append",
          "partitionBy": "[]"
         },
         null,
         [
          "2923202399434373"
         ],
         "0120-142018-2rzjaj31",
         0,
         "WriteSerializable",
         true,
         {
          "numFiles": "1",
          "numOutputBytes": "1155",
          "numOutputRows": "6"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         0,
         "2024-01-20T16:34:24.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "CREATE TABLE",
         {
          "description": null,
          "isManaged": "true",
          "partitionBy": "[]",
          "properties": "{}"
         },
         null,
         [
          "2923202399434373"
         ],
         "0120-142018-2rzjaj31",
         null,
         "WriteSerializable",
         true,
         {},
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "userId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "userName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationParameters",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "job",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "notebook",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "clusterId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "readVersion",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "isolationLevel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isBlindAppend",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "operationMetrics",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "userMetadata",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "engineInfo",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "describe history employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec36f081-fcca-42b4-a32a-0f74154af911",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>table_size_after_restore</th><th>num_of_files_after_restore</th><th>num_removed_files</th><th>num_restored_files</th><th>removed_files_size</th><th>restored_files_size</th></tr></thead><tbody><tr><td>1155</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1155</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1155,
         1,
         0,
         1,
         0,
         1155
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "table_size_after_restore",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_of_files_after_restore",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_removed_files",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_restored_files",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "removed_files_size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "restored_files_size",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "restore table employees to version as of 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c461e455-c7a4-42fd-af77-fb6d5b754dc8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>salary</th></tr></thead><tbody><tr><td>1</td><td>Adam</td><td>3500.0</td></tr><tr><td>2</td><td>Sarah</td><td>4020.5</td></tr><tr><td>3</td><td>John</td><td>2999.3</td></tr><tr><td>4</td><td>Thomas</td><td>4000.3</td></tr><tr><td>5</td><td>Anna</td><td>2500.0</td></tr><tr><td>6</td><td>Kim</td><td>6200.3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Adam",
         3500.0
        ],
        [
         2,
         "Sarah",
         4020.5
        ],
        [
         3,
         "John",
         2999.3
        ],
        [
         4,
         "Thomas",
         4000.3
        ],
        [
         5,
         "Anna",
         2500.0
        ],
        [
         6,
         "Kim",
         6200.3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a81f7835-f6f4-4a27-8cde-0be1477566e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th></tr></thead><tbody><tr><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "update employees\n",
    "set salary = salary + 100\n",
    "where \n",
    "  name like 'A%';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f0814b0-2309-454d-8220-410b49ebcdc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>salary</th></tr></thead><tbody><tr><td>1</td><td>Adam</td><td>3600.0</td></tr><tr><td>2</td><td>Sarah</td><td>4020.5</td></tr><tr><td>3</td><td>John</td><td>2999.3</td></tr><tr><td>4</td><td>Thomas</td><td>4000.3</td></tr><tr><td>5</td><td>Anna</td><td>2600.0</td></tr><tr><td>6</td><td>Kim</td><td>6200.3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Adam",
         3600.0
        ],
        [
         2,
         "Sarah",
         4020.5
        ],
        [
         3,
         "John",
         2999.3
        ],
        [
         4,
         "Thomas",
         4000.3
        ],
        [
         5,
         "Anna",
         2600.0
        ],
        [
         6,
         "Kim",
         6200.3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from employees;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a167d63d-5f3b-4c96-969d-58fbe7e81fce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>6</td><td>2024-01-20T17:26:20.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>UPDATE</td><td>Map(predicate -> [\"StartsWith(name#5544, A)\"])</td><td>null</td><td>List(2923202399434393)</td><td>0120-142018-2rzjaj31</td><td>5</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 4, numAddedChangeFiles -> 0, executionTimeMs -> 820, scanTimeMs -> 458, numAddedFiles -> 1, numUpdatedRows -> 2, rewriteTimeMs -> 361)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>5</td><td>2024-01-20T17:25:15.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>RESTORE</td><td>Map(version -> 1, timestamp -> null)</td><td>null</td><td>List(2923202399434393)</td><td>0120-142018-2rzjaj31</td><td>4</td><td>Serializable</td><td>false</td><td>Map(numRestoredFiles -> 1, removedFilesSize -> 0, numRemovedFiles -> 0, restoredFilesSize -> 1155, numOfFilesAfterRestore -> 1, tableSizeAfterRestore -> 1155)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>4</td><td>2024-01-20T17:22:23.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>TRUNCATE</td><td>Map()</td><td>null</td><td>List(2923202399434393)</td><td>0120-142018-2rzjaj31</td><td>3</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, executionTimeMs -> 99)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>3</td><td>2024-01-20T16:54:38.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>UPDATE</td><td>Map(predicate -> [\"StartsWith(name#2444, A)\"])</td><td>null</td><td>List(2923202399434373)</td><td>0120-142018-2rzjaj31</td><td>2</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 4, numAddedChangeFiles -> 0, executionTimeMs -> 902, scanTimeMs -> 461, numAddedFiles -> 1, numUpdatedRows -> 2, rewriteTimeMs -> 440)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>2</td><td>2024-01-20T16:54:05.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>UPDATE</td><td>Map(predicate -> [\"StartsWith(name#1844, A)\"])</td><td>null</td><td>List(2923202399434373)</td><td>0120-142018-2rzjaj31</td><td>1</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 4, numAddedChangeFiles -> 0, executionTimeMs -> 1623, scanTimeMs -> 1021, numAddedFiles -> 1, numUpdatedRows -> 2, rewriteTimeMs -> 592)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>1</td><td>2024-01-20T16:44:59.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>WRITE</td><td>Map(mode -> Append, partitionBy -> [])</td><td>null</td><td>List(2923202399434373)</td><td>0120-142018-2rzjaj31</td><td>0</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 1, numOutputRows -> 6, numOutputBytes -> 1155)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>0</td><td>2024-01-20T16:34:24.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>CREATE TABLE</td><td>Map(isManaged -> true, description -> null, partitionBy -> [], properties -> {})</td><td>null</td><td>List(2923202399434373)</td><td>0120-142018-2rzjaj31</td><td>null</td><td>WriteSerializable</td><td>true</td><td>Map()</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         6,
         "2024-01-20T17:26:20.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "UPDATE",
         {
          "predicate": "[\"StartsWith(name#5544, A)\"]"
         },
         null,
         [
          "2923202399434393"
         ],
         "0120-142018-2rzjaj31",
         5,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "820",
          "numAddedChangeFiles": "0",
          "numAddedFiles": "1",
          "numCopiedRows": "4",
          "numRemovedFiles": "1",
          "numUpdatedRows": "2",
          "rewriteTimeMs": "361",
          "scanTimeMs": "458"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         5,
         "2024-01-20T17:25:15.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "RESTORE",
         {
          "timestamp": null,
          "version": "1"
         },
         null,
         [
          "2923202399434393"
         ],
         "0120-142018-2rzjaj31",
         4,
         "Serializable",
         false,
         {
          "numOfFilesAfterRestore": "1",
          "numRemovedFiles": "0",
          "numRestoredFiles": "1",
          "removedFilesSize": "0",
          "restoredFilesSize": "1155",
          "tableSizeAfterRestore": "1155"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         4,
         "2024-01-20T17:22:23.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "TRUNCATE",
         {},
         null,
         [
          "2923202399434393"
         ],
         "0120-142018-2rzjaj31",
         3,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "99",
          "numRemovedFiles": "1"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         3,
         "2024-01-20T16:54:38.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "UPDATE",
         {
          "predicate": "[\"StartsWith(name#2444, A)\"]"
         },
         null,
         [
          "2923202399434373"
         ],
         "0120-142018-2rzjaj31",
         2,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "902",
          "numAddedChangeFiles": "0",
          "numAddedFiles": "1",
          "numCopiedRows": "4",
          "numRemovedFiles": "1",
          "numUpdatedRows": "2",
          "rewriteTimeMs": "440",
          "scanTimeMs": "461"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         2,
         "2024-01-20T16:54:05.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "UPDATE",
         {
          "predicate": "[\"StartsWith(name#1844, A)\"]"
         },
         null,
         [
          "2923202399434373"
         ],
         "0120-142018-2rzjaj31",
         1,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "1623",
          "numAddedChangeFiles": "0",
          "numAddedFiles": "1",
          "numCopiedRows": "4",
          "numRemovedFiles": "1",
          "numUpdatedRows": "2",
          "rewriteTimeMs": "592",
          "scanTimeMs": "1021"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         1,
         "2024-01-20T16:44:59.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "WRITE",
         {
          "mode": "Append",
          "partitionBy": "[]"
         },
         null,
         [
          "2923202399434373"
         ],
         "0120-142018-2rzjaj31",
         0,
         "WriteSerializable",
         true,
         {
          "numFiles": "1",
          "numOutputBytes": "1155",
          "numOutputRows": "6"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         0,
         "2024-01-20T16:34:24.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "CREATE TABLE",
         {
          "description": null,
          "isManaged": "true",
          "partitionBy": "[]",
          "properties": "{}"
         },
         null,
         [
          "2923202399434373"
         ],
         "0120-142018-2rzjaj31",
         null,
         "WriteSerializable",
         true,
         {},
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "userId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "userName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationParameters",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "job",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "notebook",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "clusterId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "readVersion",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "isolationLevel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isBlindAppend",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "operationMetrics",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "userMetadata",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "engineInfo",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "describe history employees;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68580011-e6ce-48c3-80ad-acd456488424",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>format</th><th>id</th><th>name</th><th>description</th><th>location</th><th>createdAt</th><th>lastModified</th><th>partitionColumns</th><th>numFiles</th><th>sizeInBytes</th><th>properties</th><th>minReaderVersion</th><th>minWriterVersion</th></tr></thead><tbody><tr><td>delta</td><td>e0957b50-bb0d-49a2-993f-0ff4a14617c1</td><td>spark_catalog.default.employees</td><td>null</td><td>dbfs:/user/hive/warehouse/employees</td><td>2024-01-20T16:34:23.876+0000</td><td>2024-01-20T17:26:20.000+0000</td><td>List()</td><td>1</td><td>1155</td><td>Map()</td><td>1</td><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "delta",
         "e0957b50-bb0d-49a2-993f-0ff4a14617c1",
         "spark_catalog.default.employees",
         null,
         "dbfs:/user/hive/warehouse/employees",
         "2024-01-20T16:34:23.876+0000",
         "2024-01-20T17:26:20.000+0000",
         [],
         1,
         1155,
         {},
         1,
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "format",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "createdAt",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "lastModified",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "partitionColumns",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "numFiles",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "sizeInBytes",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "properties",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "minReaderVersion",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "minWriterVersion",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "describe detail employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "830b3604-5361-44b6-945a-cbfddb71f6e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>metrics</th></tr></thead><tbody><tr><td>dbfs:/user/hive/warehouse/employees</td><td>List(0, 0, List(null, null, 0.0, 0, 0), List(null, null, 0.0, 0, 0), 0, List(minCubeSize(107374182400), List(0, 0), List(1, 1155), 0, List(0, 0), 0, null), 0, 1, 1, false, 0, 0, 1705772087109, 1705772088936, 4, 0, null)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/user/hive/warehouse/employees",
         [
          0,
          0,
          [
           null,
           null,
           0.0,
           0,
           0
          ],
          [
           null,
           null,
           0.0,
           0,
           0
          ],
          0,
          [
           "minCubeSize(107374182400)",
           [
            0,
            0
           ],
           [
            1,
            1155
           ],
           0,
           [
            0,
            0
           ],
           0,
           null
          ],
          0,
          1,
          1,
          false,
          0,
          0,
          1705772087109,
          1705772088936,
          4,
          0,
          null
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "metrics",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"numFilesAdded\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numFilesRemoved\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"filesAdded\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"min\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"max\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"avg\",\"type\":\"double\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalFiles\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalSize\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"filesRemoved\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"min\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"max\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"avg\",\"type\":\"double\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalFiles\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalSize\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"partitionsOptimized\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"zOrderStats\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"strategyName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"inputCubeFiles\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"num\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"inputOtherFiles\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"num\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"inputNumCubes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"mergedFiles\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"num\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"numOutputCubes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"mergedNumCubes\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"numBatches\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalConsideredFiles\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalFilesSkipped\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"preserveInsertionOrder\",\"type\":\"boolean\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numFilesSkippedToReduceWriteAmplification\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numBytesSkippedToReduceWriteAmplification\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"startTimeMs\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"endTimeMs\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalClusterParallelism\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalScheduledTasks\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"autoCompactParallelismStats\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"maxClusterActiveParallelism\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"minClusterActiveParallelism\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"maxSessionActiveParallelism\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"minSessionActiveParallelism\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}}]}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "optimize employees\n",
    "zorder by id;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9c9b109-5b4a-46f6-8e85-753be364c7ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/user/hive/warehouse/employees/_delta_log/</td><td>_delta_log/</td><td>0</td><td>1705768464000</td></tr><tr><td>dbfs:/user/hive/warehouse/employees/part-00000-05f5bee8-1d85-470a-9ee7-69fe6b3765b0-c000.snappy.parquet</td><td>part-00000-05f5bee8-1d85-470a-9ee7-69fe6b3765b0-c000.snappy.parquet</td><td>1158</td><td>1705769678000</td></tr><tr><td>dbfs:/user/hive/warehouse/employees/part-00000-4489c648-213d-464f-a134-9598d483d3ed-c000.snappy.parquet</td><td>part-00000-4489c648-213d-464f-a134-9598d483d3ed-c000.snappy.parquet</td><td>1155</td><td>1705769098000</td></tr><tr><td>dbfs:/user/hive/warehouse/employees/part-00000-84a740b7-ecc4-4ade-9cbe-fcc588c61126-c000.snappy.parquet</td><td>part-00000-84a740b7-ecc4-4ade-9cbe-fcc588c61126-c000.snappy.parquet</td><td>1155</td><td>1705771580000</td></tr><tr><td>dbfs:/user/hive/warehouse/employees/part-00000-e773309a-2d22-43b6-8f67-70b98415eb7d-c000.snappy.parquet</td><td>part-00000-e773309a-2d22-43b6-8f67-70b98415eb7d-c000.snappy.parquet</td><td>1158</td><td>1705769644000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/user/hive/warehouse/employees/_delta_log/",
         "_delta_log/",
         0,
         1705768464000
        ],
        [
         "dbfs:/user/hive/warehouse/employees/part-00000-05f5bee8-1d85-470a-9ee7-69fe6b3765b0-c000.snappy.parquet",
         "part-00000-05f5bee8-1d85-470a-9ee7-69fe6b3765b0-c000.snappy.parquet",
         1158,
         1705769678000
        ],
        [
         "dbfs:/user/hive/warehouse/employees/part-00000-4489c648-213d-464f-a134-9598d483d3ed-c000.snappy.parquet",
         "part-00000-4489c648-213d-464f-a134-9598d483d3ed-c000.snappy.parquet",
         1155,
         1705769098000
        ],
        [
         "dbfs:/user/hive/warehouse/employees/part-00000-84a740b7-ecc4-4ade-9cbe-fcc588c61126-c000.snappy.parquet",
         "part-00000-84a740b7-ecc4-4ade-9cbe-fcc588c61126-c000.snappy.parquet",
         1155,
         1705771580000
        ],
        [
         "dbfs:/user/hive/warehouse/employees/part-00000-e773309a-2d22-43b6-8f67-70b98415eb7d-c000.snappy.parquet",
         "part-00000-e773309a-2d22-43b6-8f67-70b98415eb7d-c000.snappy.parquet",
         1158,
         1705769644000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs \n",
    "ls \"dbfs:/user/hive/warehouse/employees/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31607114-73c8-4040-b103-b8c48323c31d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>6</td><td>2024-01-20T17:26:20.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>UPDATE</td><td>Map(predicate -> [\"StartsWith(name#5544, A)\"])</td><td>null</td><td>List(2923202399434393)</td><td>0120-142018-2rzjaj31</td><td>5</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 4, numAddedChangeFiles -> 0, executionTimeMs -> 820, scanTimeMs -> 458, numAddedFiles -> 1, numUpdatedRows -> 2, rewriteTimeMs -> 361)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>5</td><td>2024-01-20T17:25:15.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>RESTORE</td><td>Map(version -> 1, timestamp -> null)</td><td>null</td><td>List(2923202399434393)</td><td>0120-142018-2rzjaj31</td><td>4</td><td>Serializable</td><td>false</td><td>Map(numRestoredFiles -> 1, removedFilesSize -> 0, numRemovedFiles -> 0, restoredFilesSize -> 1155, numOfFilesAfterRestore -> 1, tableSizeAfterRestore -> 1155)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>4</td><td>2024-01-20T17:22:23.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>TRUNCATE</td><td>Map()</td><td>null</td><td>List(2923202399434393)</td><td>0120-142018-2rzjaj31</td><td>3</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, executionTimeMs -> 99)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>3</td><td>2024-01-20T16:54:38.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>UPDATE</td><td>Map(predicate -> [\"StartsWith(name#2444, A)\"])</td><td>null</td><td>List(2923202399434373)</td><td>0120-142018-2rzjaj31</td><td>2</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 4, numAddedChangeFiles -> 0, executionTimeMs -> 902, scanTimeMs -> 461, numAddedFiles -> 1, numUpdatedRows -> 2, rewriteTimeMs -> 440)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>2</td><td>2024-01-20T16:54:05.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>UPDATE</td><td>Map(predicate -> [\"StartsWith(name#1844, A)\"])</td><td>null</td><td>List(2923202399434373)</td><td>0120-142018-2rzjaj31</td><td>1</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 4, numAddedChangeFiles -> 0, executionTimeMs -> 1623, scanTimeMs -> 1021, numAddedFiles -> 1, numUpdatedRows -> 2, rewriteTimeMs -> 592)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>1</td><td>2024-01-20T16:44:59.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>WRITE</td><td>Map(mode -> Append, partitionBy -> [])</td><td>null</td><td>List(2923202399434373)</td><td>0120-142018-2rzjaj31</td><td>0</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 1, numOutputRows -> 6, numOutputBytes -> 1155)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>0</td><td>2024-01-20T16:34:24.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>CREATE TABLE</td><td>Map(isManaged -> true, description -> null, partitionBy -> [], properties -> {})</td><td>null</td><td>List(2923202399434373)</td><td>0120-142018-2rzjaj31</td><td>null</td><td>WriteSerializable</td><td>true</td><td>Map()</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         6,
         "2024-01-20T17:26:20.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "UPDATE",
         {
          "predicate": "[\"StartsWith(name#5544, A)\"]"
         },
         null,
         [
          "2923202399434393"
         ],
         "0120-142018-2rzjaj31",
         5,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "820",
          "numAddedChangeFiles": "0",
          "numAddedFiles": "1",
          "numCopiedRows": "4",
          "numRemovedFiles": "1",
          "numUpdatedRows": "2",
          "rewriteTimeMs": "361",
          "scanTimeMs": "458"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         5,
         "2024-01-20T17:25:15.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "RESTORE",
         {
          "timestamp": null,
          "version": "1"
         },
         null,
         [
          "2923202399434393"
         ],
         "0120-142018-2rzjaj31",
         4,
         "Serializable",
         false,
         {
          "numOfFilesAfterRestore": "1",
          "numRemovedFiles": "0",
          "numRestoredFiles": "1",
          "removedFilesSize": "0",
          "restoredFilesSize": "1155",
          "tableSizeAfterRestore": "1155"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         4,
         "2024-01-20T17:22:23.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "TRUNCATE",
         {},
         null,
         [
          "2923202399434393"
         ],
         "0120-142018-2rzjaj31",
         3,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "99",
          "numRemovedFiles": "1"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         3,
         "2024-01-20T16:54:38.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "UPDATE",
         {
          "predicate": "[\"StartsWith(name#2444, A)\"]"
         },
         null,
         [
          "2923202399434373"
         ],
         "0120-142018-2rzjaj31",
         2,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "902",
          "numAddedChangeFiles": "0",
          "numAddedFiles": "1",
          "numCopiedRows": "4",
          "numRemovedFiles": "1",
          "numUpdatedRows": "2",
          "rewriteTimeMs": "440",
          "scanTimeMs": "461"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         2,
         "2024-01-20T16:54:05.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "UPDATE",
         {
          "predicate": "[\"StartsWith(name#1844, A)\"]"
         },
         null,
         [
          "2923202399434373"
         ],
         "0120-142018-2rzjaj31",
         1,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "1623",
          "numAddedChangeFiles": "0",
          "numAddedFiles": "1",
          "numCopiedRows": "4",
          "numRemovedFiles": "1",
          "numUpdatedRows": "2",
          "rewriteTimeMs": "592",
          "scanTimeMs": "1021"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         1,
         "2024-01-20T16:44:59.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "WRITE",
         {
          "mode": "Append",
          "partitionBy": "[]"
         },
         null,
         [
          "2923202399434373"
         ],
         "0120-142018-2rzjaj31",
         0,
         "WriteSerializable",
         true,
         {
          "numFiles": "1",
          "numOutputBytes": "1155",
          "numOutputRows": "6"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         0,
         "2024-01-20T16:34:24.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "CREATE TABLE",
         {
          "description": null,
          "isManaged": "true",
          "partitionBy": "[]",
          "properties": "{}"
         },
         null,
         [
          "2923202399434373"
         ],
         "0120-142018-2rzjaj31",
         null,
         "WriteSerializable",
         true,
         {},
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "userId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "userName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationParameters",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "job",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "notebook",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "clusterId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "readVersion",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "isolationLevel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isBlindAppend",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "operationMetrics",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "userMetadata",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "engineInfo",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "describe history employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f67ab8d8-37ea-4585-be2d-936f1977d5dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th></tr></thead><tbody><tr><td>dbfs:/user/hive/warehouse/employees</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/user/hive/warehouse/employees"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "vacuum employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58805c56-d016-4e6a-baf7-fa574eff1868",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>8</td><td>2024-01-20T17:43:41.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>VACUUM END</td><td>Map(status -> COMPLETED)</td><td>null</td><td>List(2923202399434393)</td><td>0120-142018-2rzjaj31</td><td>7</td><td>SnapshotIsolation</td><td>true</td><td>Map(numDeletedFiles -> 0, numVacuumedDirectories -> 1)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>7</td><td>2024-01-20T17:43:39.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>VACUUM START</td><td>Map(retentionCheckEnabled -> true, defaultRetentionMillis -> 604800000)</td><td>null</td><td>List(2923202399434393)</td><td>0120-142018-2rzjaj31</td><td>6</td><td>SnapshotIsolation</td><td>true</td><td>Map(numFilesToDelete -> 0, sizeOfDataToDelete -> 0)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>6</td><td>2024-01-20T17:26:20.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>UPDATE</td><td>Map(predicate -> [\"StartsWith(name#5544, A)\"])</td><td>null</td><td>List(2923202399434393)</td><td>0120-142018-2rzjaj31</td><td>5</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 4, numAddedChangeFiles -> 0, executionTimeMs -> 820, scanTimeMs -> 458, numAddedFiles -> 1, numUpdatedRows -> 2, rewriteTimeMs -> 361)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>5</td><td>2024-01-20T17:25:15.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>RESTORE</td><td>Map(version -> 1, timestamp -> null)</td><td>null</td><td>List(2923202399434393)</td><td>0120-142018-2rzjaj31</td><td>4</td><td>Serializable</td><td>false</td><td>Map(numRestoredFiles -> 1, removedFilesSize -> 0, numRemovedFiles -> 0, restoredFilesSize -> 1155, numOfFilesAfterRestore -> 1, tableSizeAfterRestore -> 1155)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>4</td><td>2024-01-20T17:22:23.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>TRUNCATE</td><td>Map()</td><td>null</td><td>List(2923202399434393)</td><td>0120-142018-2rzjaj31</td><td>3</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, executionTimeMs -> 99)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>3</td><td>2024-01-20T16:54:38.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>UPDATE</td><td>Map(predicate -> [\"StartsWith(name#2444, A)\"])</td><td>null</td><td>List(2923202399434373)</td><td>0120-142018-2rzjaj31</td><td>2</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 4, numAddedChangeFiles -> 0, executionTimeMs -> 902, scanTimeMs -> 461, numAddedFiles -> 1, numUpdatedRows -> 2, rewriteTimeMs -> 440)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>2</td><td>2024-01-20T16:54:05.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>UPDATE</td><td>Map(predicate -> [\"StartsWith(name#1844, A)\"])</td><td>null</td><td>List(2923202399434373)</td><td>0120-142018-2rzjaj31</td><td>1</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 4, numAddedChangeFiles -> 0, executionTimeMs -> 1623, scanTimeMs -> 1021, numAddedFiles -> 1, numUpdatedRows -> 2, rewriteTimeMs -> 592)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>1</td><td>2024-01-20T16:44:59.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>WRITE</td><td>Map(mode -> Append, partitionBy -> [])</td><td>null</td><td>List(2923202399434373)</td><td>0120-142018-2rzjaj31</td><td>0</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 1, numOutputRows -> 6, numOutputBytes -> 1155)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>0</td><td>2024-01-20T16:34:24.000+0000</td><td>4582792211429408</td><td>ali.muhammad@blutechconsulting.com</td><td>CREATE TABLE</td><td>Map(isManaged -> true, description -> null, partitionBy -> [], properties -> {})</td><td>null</td><td>List(2923202399434373)</td><td>0120-142018-2rzjaj31</td><td>null</td><td>WriteSerializable</td><td>true</td><td>Map()</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         8,
         "2024-01-20T17:43:41.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "VACUUM END",
         {
          "status": "COMPLETED"
         },
         null,
         [
          "2923202399434393"
         ],
         "0120-142018-2rzjaj31",
         7,
         "SnapshotIsolation",
         true,
         {
          "numDeletedFiles": "0",
          "numVacuumedDirectories": "1"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         7,
         "2024-01-20T17:43:39.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "VACUUM START",
         {
          "defaultRetentionMillis": "604800000",
          "retentionCheckEnabled": "true"
         },
         null,
         [
          "2923202399434393"
         ],
         "0120-142018-2rzjaj31",
         6,
         "SnapshotIsolation",
         true,
         {
          "numFilesToDelete": "0",
          "sizeOfDataToDelete": "0"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         6,
         "2024-01-20T17:26:20.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "UPDATE",
         {
          "predicate": "[\"StartsWith(name#5544, A)\"]"
         },
         null,
         [
          "2923202399434393"
         ],
         "0120-142018-2rzjaj31",
         5,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "820",
          "numAddedChangeFiles": "0",
          "numAddedFiles": "1",
          "numCopiedRows": "4",
          "numRemovedFiles": "1",
          "numUpdatedRows": "2",
          "rewriteTimeMs": "361",
          "scanTimeMs": "458"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         5,
         "2024-01-20T17:25:15.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "RESTORE",
         {
          "timestamp": null,
          "version": "1"
         },
         null,
         [
          "2923202399434393"
         ],
         "0120-142018-2rzjaj31",
         4,
         "Serializable",
         false,
         {
          "numOfFilesAfterRestore": "1",
          "numRemovedFiles": "0",
          "numRestoredFiles": "1",
          "removedFilesSize": "0",
          "restoredFilesSize": "1155",
          "tableSizeAfterRestore": "1155"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         4,
         "2024-01-20T17:22:23.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "TRUNCATE",
         {},
         null,
         [
          "2923202399434393"
         ],
         "0120-142018-2rzjaj31",
         3,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "99",
          "numRemovedFiles": "1"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         3,
         "2024-01-20T16:54:38.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "UPDATE",
         {
          "predicate": "[\"StartsWith(name#2444, A)\"]"
         },
         null,
         [
          "2923202399434373"
         ],
         "0120-142018-2rzjaj31",
         2,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "902",
          "numAddedChangeFiles": "0",
          "numAddedFiles": "1",
          "numCopiedRows": "4",
          "numRemovedFiles": "1",
          "numUpdatedRows": "2",
          "rewriteTimeMs": "440",
          "scanTimeMs": "461"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         2,
         "2024-01-20T16:54:05.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "UPDATE",
         {
          "predicate": "[\"StartsWith(name#1844, A)\"]"
         },
         null,
         [
          "2923202399434373"
         ],
         "0120-142018-2rzjaj31",
         1,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "1623",
          "numAddedChangeFiles": "0",
          "numAddedFiles": "1",
          "numCopiedRows": "4",
          "numRemovedFiles": "1",
          "numUpdatedRows": "2",
          "rewriteTimeMs": "592",
          "scanTimeMs": "1021"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         1,
         "2024-01-20T16:44:59.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "WRITE",
         {
          "mode": "Append",
          "partitionBy": "[]"
         },
         null,
         [
          "2923202399434373"
         ],
         "0120-142018-2rzjaj31",
         0,
         "WriteSerializable",
         true,
         {
          "numFiles": "1",
          "numOutputBytes": "1155",
          "numOutputRows": "6"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         0,
         "2024-01-20T16:34:24.000+0000",
         "4582792211429408",
         "ali.muhammad@blutechconsulting.com",
         "CREATE TABLE",
         {
          "description": null,
          "isManaged": "true",
          "partitionBy": "[]",
          "properties": "{}"
         },
         null,
         [
          "2923202399434373"
         ],
         "0120-142018-2rzjaj31",
         null,
         "WriteSerializable",
         true,
         {},
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "userId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "userName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationParameters",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "job",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "notebook",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "clusterId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "readVersion",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "isolationLevel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isBlindAppend",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "operationMetrics",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "userMetadata",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "engineInfo",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "describe history employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ed44c9a-02c3-4d3c-a91f-dd66fdad7973",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/user/hive/warehouse/employees/_delta_log/</td><td>_delta_log/</td><td>0</td><td>1705768464000</td></tr><tr><td>dbfs:/user/hive/warehouse/employees/part-00000-05f5bee8-1d85-470a-9ee7-69fe6b3765b0-c000.snappy.parquet</td><td>part-00000-05f5bee8-1d85-470a-9ee7-69fe6b3765b0-c000.snappy.parquet</td><td>1158</td><td>1705769678000</td></tr><tr><td>dbfs:/user/hive/warehouse/employees/part-00000-4489c648-213d-464f-a134-9598d483d3ed-c000.snappy.parquet</td><td>part-00000-4489c648-213d-464f-a134-9598d483d3ed-c000.snappy.parquet</td><td>1155</td><td>1705769098000</td></tr><tr><td>dbfs:/user/hive/warehouse/employees/part-00000-84a740b7-ecc4-4ade-9cbe-fcc588c61126-c000.snappy.parquet</td><td>part-00000-84a740b7-ecc4-4ade-9cbe-fcc588c61126-c000.snappy.parquet</td><td>1155</td><td>1705771580000</td></tr><tr><td>dbfs:/user/hive/warehouse/employees/part-00000-e773309a-2d22-43b6-8f67-70b98415eb7d-c000.snappy.parquet</td><td>part-00000-e773309a-2d22-43b6-8f67-70b98415eb7d-c000.snappy.parquet</td><td>1158</td><td>1705769644000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/user/hive/warehouse/employees/_delta_log/",
         "_delta_log/",
         0,
         1705768464000
        ],
        [
         "dbfs:/user/hive/warehouse/employees/part-00000-05f5bee8-1d85-470a-9ee7-69fe6b3765b0-c000.snappy.parquet",
         "part-00000-05f5bee8-1d85-470a-9ee7-69fe6b3765b0-c000.snappy.parquet",
         1158,
         1705769678000
        ],
        [
         "dbfs:/user/hive/warehouse/employees/part-00000-4489c648-213d-464f-a134-9598d483d3ed-c000.snappy.parquet",
         "part-00000-4489c648-213d-464f-a134-9598d483d3ed-c000.snappy.parquet",
         1155,
         1705769098000
        ],
        [
         "dbfs:/user/hive/warehouse/employees/part-00000-84a740b7-ecc4-4ade-9cbe-fcc588c61126-c000.snappy.parquet",
         "part-00000-84a740b7-ecc4-4ade-9cbe-fcc588c61126-c000.snappy.parquet",
         1155,
         1705771580000
        ],
        [
         "dbfs:/user/hive/warehouse/employees/part-00000-e773309a-2d22-43b6-8f67-70b98415eb7d-c000.snappy.parquet",
         "part-00000-e773309a-2d22-43b6-8f67-70b98415eb7d-c000.snappy.parquet",
         1158,
         1705769644000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs \n",
    "ls \"dbfs:/user/hive/warehouse/employees/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10d7afbe-1298-4ec1-825f-b98b43b6e1fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>key</th><th>value</th></tr></thead><tbody><tr><td>spark.databricks.delta.retentionDurationCheck.enabled</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "spark.databricks.delta.retentionDurationCheck.enabled",
         "false"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "key",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SET spark.databricks.delta.retentionDurationCheck.enabled = false;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60e08286-edd9-45bd-8459-ea5dede2a07f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th></tr></thead><tbody><tr><td>dbfs:/user/hive/warehouse/employees</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/user/hive/warehouse/employees"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "vacuum employees retain 0 hours;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b99dfb-8af9-4ee9-bf72-b1c533cb865e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/user/hive/warehouse/employees/_delta_log/</td><td>_delta_log/</td><td>0</td><td>1705768464000</td></tr><tr><td>dbfs:/user/hive/warehouse/employees/part-00000-84a740b7-ecc4-4ade-9cbe-fcc588c61126-c000.snappy.parquet</td><td>part-00000-84a740b7-ecc4-4ade-9cbe-fcc588c61126-c000.snappy.parquet</td><td>1155</td><td>1705771580000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/user/hive/warehouse/employees/_delta_log/",
         "_delta_log/",
         0,
         1705768464000
        ],
        [
         "dbfs:/user/hive/warehouse/employees/part-00000-84a740b7-ecc4-4ade-9cbe-fcc588c61126-c000.snappy.parquet",
         "part-00000-84a740b7-ecc4-4ade-9cbe-fcc588c61126-c000.snappy.parquet",
         1155,
         1705771580000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs \n",
    "ls \"dbfs:/user/hive/warehouse/employees/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f38e95d-4d7f-4b31-a7e2-473cd87ecebd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 275.0 failed 4 times, most recent failure: Lost task 0.3 in stage 275.0 (TID 2408) (10.139.64.4 executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/user/hive/warehouse/employees/part-00000-4489c648-213d-464f-a134-9598d483d3ed-c000.snappy.parquet. File dbfs:/user/hive/warehouse/employees/part-00000-4489c648-213d-464f-a134-9598d483d3ed-c000.snappy.parquet referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.microsoft.com/azure/databricks/delta/delta-intro#frequently-asked-questions\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:654)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:602)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:747)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:456)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:451)\n",
       "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:1983)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:186)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:137)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:137)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: java.io.FileNotFoundException: Operation failed: \"The specified path does not exist.\", 404, GET, https://dbstorage7f3xd75jcyv2o.dfs.core.windows.net/root/1460117410888808/user/hive/warehouse/employees/part-00000-4489c648-213d-464f-a134-9598d483d3ed-c000.snappy.parquet?timeout=90&sig=XXXX&st=2024-01-20T16%3A01%3A28Z&se=2024-01-21T12%3A01%3A28Z&sv=2019-02-02&spr=https&sp=racwdl&sr=c, PathNotFound, \"The specified path does not exist. RequestId:c9b55a10-001f-002b-5cc9-4b0afc000000 Time:2024-01-20T17:52:40.1980281Z\"\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readRemote(AbfsInputStream.java:617)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readInternal(AbfsInputStream.java:566)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readOneBlock(AbfsInputStream.java:360)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:298)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:265)\n",
       "\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n",
       "\tat com.databricks.spark.metrics.FSInputStreamWithMetrics.$anonfun$read$2(FileSystemWithMetrics.scala:214)\n",
       "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
       "\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeMetric(FileSystemWithMetrics.scala:158)\n",
       "\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeAndBytesMetric(FileSystemWithMetrics.scala:178)\n",
       "\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.$anonfun$withTimeAndBytesReadMetric$1(FileSystemWithMetrics.scala:192)\n",
       "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
       "\tat com.databricks.spark.metrics.SamplerWithPeriod.sample(FileSystemWithMetrics.scala:85)\n",
       "\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeAndBytesReadMetric(FileSystemWithMetrics.scala:192)\n",
       "\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeAndBytesReadMetric$(FileSystemWithMetrics.scala:191)\n",
       "\tat com.databricks.spark.metrics.FSInputStreamWithMetrics.withTimeAndBytesReadMetric(FileSystemWithMetrics.scala:199)\n",
       "\tat com.databricks.spark.metrics.FSInputStreamWithMetrics.read(FileSystemWithMetrics.scala:213)\n",
       "\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n",
       "\tat org.apache.parquet.bytes.BytesUtils.readIntLittleEndian(BytesUtils.java:83)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooterBytesFromInputStream(CachingParquetFooterReader.java:135)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFileReader$FooterByteReader.read(CachingParquetFileReader.java:374)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader$LegacyCache.read(CachingParquetFooterReader.java:338)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.lambda$readFooterFromStorage$0(CachingParquetFooterReader.java:286)\n",
       "\tat org.apache.spark.util.JavaFrameProfiler.record(JavaFrameProfiler.java:17)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooterFromStorage(CachingParquetFooterReader.java:282)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooter(CachingParquetFooterReader.java:204)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:396)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:157)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$2.apply(ParquetFileFormat.scala:502)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$2.apply(ParquetFileFormat.scala:356)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:573)\n",
       "\t... 33 more\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3381)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3313)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3304)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3304)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1428)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1428)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1428)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3593)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3531)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3519)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1177)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1165)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2746)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:312)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:271)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:322)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:105)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:112)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:115)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:104)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:88)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:527)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:519)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:539)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:396)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:390)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:292)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:433)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:430)\n",
       "\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3431)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$collectResult$1(Dataset.scala:3422)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4297)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:773)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4295)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:249)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:399)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:194)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:148)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:349)\n",
       "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4295)\n",
       "\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3421)\n",
       "\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:267)\n",
       "\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:101)\n",
       "\tat com.databricks.backend.daemon.driver.PythonDriverLocalBase.generateTableResult(PythonDriverLocalBase.scala:723)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.computeListResultsItem(JupyterDriverLocal.scala:1424)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal$JupyterEntryPoint.addCustomDisplayData(JupyterDriverLocal.scala:505)\n",
       "\tat sun.reflect.GeneratedMethodAccessor545.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:/user/hive/warehouse/employees/part-00000-4489c648-213d-464f-a134-9598d483d3ed-c000.snappy.parquet. File dbfs:/user/hive/warehouse/employees/part-00000-4489c648-213d-464f-a134-9598d483d3ed-c000.snappy.parquet referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.microsoft.com/azure/databricks/delta/delta-intro#frequently-asked-questions\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:654)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:602)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:747)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:456)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:451)\n",
       "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:1983)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:186)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:137)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:137)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\t... 1 more\n",
       "Caused by: java.io.FileNotFoundException: Operation failed: \"The specified path does not exist.\", 404, GET, https://dbstorage7f3xd75jcyv2o.dfs.core.windows.net/root/1460117410888808/user/hive/warehouse/employees/part-00000-4489c648-213d-464f-a134-9598d483d3ed-c000.snappy.parquet?timeout=90&sig=XXXX&st=2024-01-20T16%3A01%3A28Z&se=2024-01-21T12%3A01%3A28Z&sv=2019-02-02&spr=https&sp=racwdl&sr=c, PathNotFound, \"The specified path does not exist. RequestId:c9b55a10-001f-002b-5cc9-4b0afc000000 Time:2024-01-20T17:52:40.1980281Z\"\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readRemote(AbfsInputStream.java:617)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readInternal(AbfsInputStream.java:566)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readOneBlock(AbfsInputStream.java:360)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:298)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:265)\n",
       "\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n",
       "\tat com.databricks.spark.metrics.FSInputStreamWithMetrics.$anonfun$read$2(FileSystemWithMetrics.scala:214)\n",
       "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
       "\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeMetric(FileSystemWithMetrics.scala:158)\n",
       "\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeAndBytesMetric(FileSystemWithMetrics.scala:178)\n",
       "\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.$anonfun$withTimeAndBytesReadMetric$1(FileSystemWithMetrics.scala:192)\n",
       "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
       "\tat com.databricks.spark.metrics.SamplerWithPeriod.sample(FileSystemWithMetrics.scala:85)\n",
       "\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeAndBytesReadMetric(FileSystemWithMetrics.scala:192)\n",
       "\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeAndBytesReadMetric$(FileSystemWithMetrics.scala:191)\n",
       "\tat com.databricks.spark.metrics.FSInputStreamWithMetrics.withTimeAndBytesReadMetric(FileSystemWithMetrics.scala:199)\n",
       "\tat com.databricks.spark.metrics.FSInputStreamWithMetrics.read(FileSystemWithMetrics.scala:213)\n",
       "\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n",
       "\tat org.apache.parquet.bytes.BytesUtils.readIntLittleEndian(BytesUtils.java:83)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooterBytesFromInputStream(CachingParquetFooterReader.java:135)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFileReader$FooterByteReader.read(CachingParquetFileReader.java:374)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader$LegacyCache.read(CachingParquetFooterReader.java:338)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.lambda$readFooterFromStorage$0(CachingParquetFooterReader.java:286)\n",
       "\tat org.apache.spark.util.JavaFrameProfiler.record(JavaFrameProfiler.java:17)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooterFromStorage(CachingParquetFooterReader.java:282)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooter(CachingParquetFooterReader.java:204)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:396)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:157)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$2.apply(ParquetFileFormat.scala:502)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$2.apply(ParquetFileFormat.scala:356)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:573)\n",
       "\t... 33 more"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 275.0 failed 4 times, most recent failure: Lost task 0.3 in stage 275.0 (TID 2408) (10.139.64.4 executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/user/hive/warehouse/employees/part-00000-4489c648-213d-464f-a134-9598d483d3ed-c000.snappy.parquet. File dbfs:/user/hive/warehouse/employees/part-00000-4489c648-213d-464f-a134-9598d483d3ed-c000.snappy.parquet referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.microsoft.com/azure/databricks/delta/delta-intro#frequently-asked-questions\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:654)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:602)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:747)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:456)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:451)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:1983)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:186)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:137)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:137)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.FileNotFoundException: Operation failed: \"The specified path does not exist.\", 404, GET, https://dbstorage7f3xd75jcyv2o.dfs.core.windows.net/root/1460117410888808/user/hive/warehouse/employees/part-00000-4489c648-213d-464f-a134-9598d483d3ed-c000.snappy.parquet?timeout=90&sig=XXXX&st=2024-01-20T16%3A01%3A28Z&se=2024-01-21T12%3A01%3A28Z&sv=2019-02-02&spr=https&sp=racwdl&sr=c, PathNotFound, \"The specified path does not exist. RequestId:c9b55a10-001f-002b-5cc9-4b0afc000000 Time:2024-01-20T17:52:40.1980281Z\"\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readRemote(AbfsInputStream.java:617)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readInternal(AbfsInputStream.java:566)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readOneBlock(AbfsInputStream.java:360)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:298)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat com.databricks.spark.metrics.FSInputStreamWithMetrics.$anonfun$read$2(FileSystemWithMetrics.scala:214)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeMetric(FileSystemWithMetrics.scala:158)\n\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeAndBytesMetric(FileSystemWithMetrics.scala:178)\n\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.$anonfun$withTimeAndBytesReadMetric$1(FileSystemWithMetrics.scala:192)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat com.databricks.spark.metrics.SamplerWithPeriod.sample(FileSystemWithMetrics.scala:85)\n\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeAndBytesReadMetric(FileSystemWithMetrics.scala:192)\n\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeAndBytesReadMetric$(FileSystemWithMetrics.scala:191)\n\tat com.databricks.spark.metrics.FSInputStreamWithMetrics.withTimeAndBytesReadMetric(FileSystemWithMetrics.scala:199)\n\tat com.databricks.spark.metrics.FSInputStreamWithMetrics.read(FileSystemWithMetrics.scala:213)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.parquet.bytes.BytesUtils.readIntLittleEndian(BytesUtils.java:83)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooterBytesFromInputStream(CachingParquetFooterReader.java:135)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader$FooterByteReader.read(CachingParquetFileReader.java:374)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader$LegacyCache.read(CachingParquetFooterReader.java:338)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.lambda$readFooterFromStorage$0(CachingParquetFooterReader.java:286)\n\tat org.apache.spark.util.JavaFrameProfiler.record(JavaFrameProfiler.java:17)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooterFromStorage(CachingParquetFooterReader.java:282)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooter(CachingParquetFooterReader.java:204)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:396)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:157)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$2.apply(ParquetFileFormat.scala:502)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$2.apply(ParquetFileFormat.scala:356)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:573)\n\t... 33 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3381)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3313)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3304)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3304)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1428)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1428)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1428)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3593)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3531)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3519)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1177)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1165)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2746)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:312)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:271)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:322)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:105)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:112)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:115)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:104)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:88)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:527)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:519)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:539)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:396)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:390)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:292)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:433)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:430)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectResult$1(Dataset.scala:3422)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4297)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:773)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4295)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:249)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:399)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:194)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:148)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:349)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4295)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3421)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:267)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:101)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocalBase.generateTableResult(PythonDriverLocalBase.scala:723)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.computeListResultsItem(JupyterDriverLocal.scala:1424)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal$JupyterEntryPoint.addCustomDisplayData(JupyterDriverLocal.scala:505)\n\tat sun.reflect.GeneratedMethodAccessor545.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:/user/hive/warehouse/employees/part-00000-4489c648-213d-464f-a134-9598d483d3ed-c000.snappy.parquet. File dbfs:/user/hive/warehouse/employees/part-00000-4489c648-213d-464f-a134-9598d483d3ed-c000.snappy.parquet referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.microsoft.com/azure/databricks/delta/delta-intro#frequently-asked-questions\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:654)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:602)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:747)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:456)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:451)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:1983)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:186)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:137)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:137)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.FileNotFoundException: Operation failed: \"The specified path does not exist.\", 404, GET, https://dbstorage7f3xd75jcyv2o.dfs.core.windows.net/root/1460117410888808/user/hive/warehouse/employees/part-00000-4489c648-213d-464f-a134-9598d483d3ed-c000.snappy.parquet?timeout=90&sig=XXXX&st=2024-01-20T16%3A01%3A28Z&se=2024-01-21T12%3A01%3A28Z&sv=2019-02-02&spr=https&sp=racwdl&sr=c, PathNotFound, \"The specified path does not exist. RequestId:c9b55a10-001f-002b-5cc9-4b0afc000000 Time:2024-01-20T17:52:40.1980281Z\"\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readRemote(AbfsInputStream.java:617)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readInternal(AbfsInputStream.java:566)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readOneBlock(AbfsInputStream.java:360)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:298)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat com.databricks.spark.metrics.FSInputStreamWithMetrics.$anonfun$read$2(FileSystemWithMetrics.scala:214)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeMetric(FileSystemWithMetrics.scala:158)\n\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeAndBytesMetric(FileSystemWithMetrics.scala:178)\n\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.$anonfun$withTimeAndBytesReadMetric$1(FileSystemWithMetrics.scala:192)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat com.databricks.spark.metrics.SamplerWithPeriod.sample(FileSystemWithMetrics.scala:85)\n\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeAndBytesReadMetric(FileSystemWithMetrics.scala:192)\n\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeAndBytesReadMetric$(FileSystemWithMetrics.scala:191)\n\tat com.databricks.spark.metrics.FSInputStreamWithMetrics.withTimeAndBytesReadMetric(FileSystemWithMetrics.scala:199)\n\tat com.databricks.spark.metrics.FSInputStreamWithMetrics.read(FileSystemWithMetrics.scala:213)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.parquet.bytes.BytesUtils.readIntLittleEndian(BytesUtils.java:83)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooterBytesFromInputStream(CachingParquetFooterReader.java:135)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader$FooterByteReader.read(CachingParquetFileReader.java:374)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader$LegacyCache.read(CachingParquetFooterReader.java:338)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.lambda$readFooterFromStorage$0(CachingParquetFooterReader.java:286)\n\tat org.apache.spark.util.JavaFrameProfiler.record(JavaFrameProfiler.java:17)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooterFromStorage(CachingParquetFooterReader.java:282)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooter(CachingParquetFooterReader.java:204)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:396)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:157)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$2.apply(ParquetFileFormat.scala:502)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$2.apply(ParquetFileFormat.scala:356)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:573)\n\t... 33 more\n",
       "errorSummary": "FileReadException: Error while reading file dbfs:/user/hive/warehouse/employees/part-00000-4489c648-213d-464f-a134-9598d483d3ed-c000.snappy.parquet. File dbfs:/user/hive/warehouse/employees/part-00000-4489c648-213d-464f-a134-9598d483d3ed-c000.snappy.parquet referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.microsoft.com/azure/databricks/delta/delta-intro#frequently-asked-questions\nCaused by: FileNotFoundException: Operation failed: \"The specified path does not exist.\", 404, GET, https://dbstorage7f3xd75jcyv2o.dfs.core.windows.net/root/1460117410888808/user/hive/warehouse/employees/part-00000-4489c648-213d-464f-a134-9598d483d3ed-c000.snappy.parquet?timeout=90&sig=XXXX&st=2024-01-20T16%3A01%3A28Z&se=2024-01-21T12%3A01%3A28Z&sv=2019-02-02&spr=https&sp=racwdl&sr=c, PathNotFound, \"The specified path does not exist. RequestId:c9b55a10-001f-002b-5cc9-4b0afc000000 Time:2024-01-20T17:52:40.1980281Z\"",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from employees@v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7982215-f85b-451f-97e4-4d9415f3b690",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "drop table employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3343e212-d989-4bd0-9135-d786c06acb11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "\u001B[0;32m<command-2923202399434417>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m     \u001B[0mdisplay\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m----> 7\u001B[0;31m   \u001B[0m_sqldf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m____databricks_percent_sql\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m      8\u001B[0m \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      9\u001B[0m   \u001B[0;32mdel\u001B[0m \u001B[0m____databricks_percent_sql\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m<command-2923202399434417>\u001B[0m in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m____databricks_percent_sql\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      3\u001B[0m     \u001B[0;32mimport\u001B[0m \u001B[0mbase64\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m----> 4\u001B[0;31m     \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbase64\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstandard_b64decode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"c2VsZWN0ICogZnJvbSBlbXBsb3llZXM=\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdecode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m      5\u001B[0m     \u001B[0mdisplay\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n",
       "\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36msql\u001B[0;34m(self, sqlQuery, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1117\u001B[0m             \u001B[0msqlQuery\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mformatter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msqlQuery\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1118\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m-> 1119\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msqlQuery\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m   1120\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1121\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n",
       "\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n",
       "\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Table or view not found: employees; line 1 pos 14;\n",
       "'Project [*]\n",
       "+- 'UnresolvedRelation [employees], [], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-2923202399434417>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mdisplay\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m   \u001B[0m_sqldf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m____databricks_percent_sql\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m   \u001B[0;32mdel\u001B[0m \u001B[0m____databricks_percent_sql\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m<command-2923202399434417>\u001B[0m in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m____databricks_percent_sql\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0;32mimport\u001B[0m \u001B[0mbase64\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m     \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbase64\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstandard_b64decode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"c2VsZWN0ICogZnJvbSBlbXBsb3llZXM=\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdecode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m     \u001B[0mdisplay\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36msql\u001B[0;34m(self, sqlQuery, **kwargs)\u001B[0m\n\u001B[1;32m   1117\u001B[0m             \u001B[0msqlQuery\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mformatter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msqlQuery\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1118\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1119\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msqlQuery\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1120\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1121\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Table or view not found: employees; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [employees], [], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Table or view not found: employees; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [employees], [], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from employees"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2923202399434417,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Advance Delta Lake Features",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
